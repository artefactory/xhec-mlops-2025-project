{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries and Setup\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set figure size defaults\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "\n",
    "print(\"📊 Abalone Age Prediction - Comprehensive EDA\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries and Setup\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set figure size defaults\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "\n",
    "print(\"📊 Abalone Age Prediction - Comprehensive EDA\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load and Initial Data Inspection\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/abalone.csv\")\n",
    "\n",
    "print(\"🔍 DATASET OVERVIEW\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\n📋 Column Information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n🎯 First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Quality Assessment\n",
    "print(\"🔍 DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\"Missing Count\": missing_values, \"Missing Percentage\": missing_percent}).sort_values(\n",
    "    \"Missing Count\", ascending=False\n",
    ")\n",
    "print(missing_df)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\n🔄 Duplicate rows: {duplicates}\")\n",
    "\n",
    "# Data types\n",
    "print(f\"\\n📊 Data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Unique values in categorical columns\n",
    "categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "print(f\"\\n🏷️ Categorical columns unique values:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"{col}: {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Basic Statistical Summary\n",
    "print(\"📈 STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Basic statistics for numerical columns\n",
    "print(\"Numerical columns summary:\")\n",
    "numerical_summary = df.describe()\n",
    "print(numerical_summary)\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\n📊 Additional Statistics:\")\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "stats_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Skewness\": df[numerical_cols].skew(),\n",
    "        \"Kurtosis\": df[numerical_cols].kurtosis(),\n",
    "        \"Variance\": df[numerical_cols].var(),\n",
    "        \"Range\": df[numerical_cols].max() - df[numerical_cols].min(),\n",
    "    }\n",
    ")\n",
    "print(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Target Variable Analysis (Rings/Age)\n",
    "print(\"🎯 TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Assuming 'Rings' is the target variable (age indicator)\n",
    "target_col = \"Rings\" if \"Rings\" in df.columns else df.columns[-1]\n",
    "print(f\"Target variable: {target_col}\")\n",
    "\n",
    "# Target distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Histogram\n",
    "axes[0, 0].hist(df[target_col], bins=30, alpha=0.7, color=\"skyblue\", edgecolor=\"black\")\n",
    "axes[0, 0].axvline(df[target_col].mean(), color=\"red\", linestyle=\"--\", label=f\"Mean: {df[target_col].mean():.2f}\")\n",
    "axes[0, 0].axvline(df[target_col].median(), color=\"green\", linestyle=\"--\", label=f\"Median: {df[target_col].median():.2f}\")\n",
    "axes[0, 0].set_title(f\"Distribution of {target_col}\")\n",
    "axes[0, 0].set_xlabel(target_col)\n",
    "axes[0, 0].set_ylabel(\"Frequency\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[0, 1].boxplot(df[target_col])\n",
    "axes[0, 1].set_title(f\"Box Plot of {target_col}\")\n",
    "axes[0, 1].set_ylabel(target_col)\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(df[target_col], dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title(f\"Q-Q Plot of {target_col}\")\n",
    "\n",
    "# Density plot\n",
    "df[target_col].plot(kind=\"density\", ax=axes[1, 1], color=\"purple\")\n",
    "axes[1, 1].set_title(f\"Density Plot of {target_col}\")\n",
    "axes[1, 1].set_xlabel(target_col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Target statistics\n",
    "print(f\"\\n📊 {target_col} Statistics:\")\n",
    "print(f\"Mean: {df[target_col].mean():.2f}\")\n",
    "print(f\"Median: {df[target_col].median():.2f}\")\n",
    "print(f\"Mode: {df[target_col].mode().iloc[0]:.2f}\")\n",
    "print(f\"Standard Deviation: {df[target_col].std():.2f}\")\n",
    "print(f\"Skewness: {df[target_col].skew():.2f}\")\n",
    "print(f\"Kurtosis: {df[target_col].kurtosis():.2f}\")\n",
    "print(f\"Range: {df[target_col].min():.2f} - {df[target_col].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Categorical Variable Analysis\n",
    "print(\"🏷️ CATEGORICAL VARIABLES ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\n📊 Analysis of {col}:\")\n",
    "\n",
    "        # Value counts\n",
    "        value_counts = df[col].value_counts()\n",
    "        print(\"Value counts:\")\n",
    "        print(value_counts)\n",
    "\n",
    "        # Proportions\n",
    "        proportions = df[col].value_counts(normalize=True) * 100\n",
    "        print(\"\\nProportions (%):\")\n",
    "        print(proportions.round(2))\n",
    "\n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "        # Bar plot\n",
    "        value_counts.plot(kind=\"bar\", ax=axes[0], color=\"lightcoral\")\n",
    "        axes[0].set_title(f\"Count of {col}\")\n",
    "        axes[0].set_xlabel(col)\n",
    "        axes[0].set_ylabel(\"Count\")\n",
    "        axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        # Pie chart\n",
    "        axes[1].pie(value_counts.values, labels=value_counts.index, autopct=\"%1.1f%%\", startangle=90)\n",
    "        axes[1].set_title(f\"Distribution of {col}\")\n",
    "\n",
    "        # Target by category\n",
    "        if target_col in df.columns:\n",
    "            df.groupby(col)[target_col].mean().plot(kind=\"bar\", ax=axes[2], color=\"lightgreen\")\n",
    "            axes[2].set_title(f\"Average {target_col} by {col}\")\n",
    "            axes[2].set_xlabel(col)\n",
    "            axes[2].set_ylabel(f\"Average {target_col}\")\n",
    "            axes[2].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No categorical variables found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Numerical Variables Distribution Analysis\n",
    "print(\"📊 NUMERICAL VARIABLES DISTRIBUTION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "n_cols = len(numerical_cols)\n",
    "\n",
    "# Create subplots for histograms\n",
    "n_rows = (n_cols + 2) // 3\n",
    "fig, axes = plt.subplots(n_rows, 3, figsize=(18, 6 * n_rows))\n",
    "axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else axes\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    if i < len(axes):\n",
    "        # Histogram with KDE\n",
    "        axes[i].hist(df[col], bins=30, alpha=0.7, density=True, color=\"skyblue\", edgecolor=\"black\")\n",
    "\n",
    "        # Add KDE curve\n",
    "        df[col].plot(kind=\"density\", ax=axes[i], color=\"red\", linewidth=2)\n",
    "\n",
    "        # Add mean and median lines\n",
    "        axes[i].axvline(df[col].mean(), color=\"orange\", linestyle=\"--\", label=f\"Mean: {df[col].mean():.2f}\")\n",
    "        axes[i].axvline(df[col].median(), color=\"green\", linestyle=\"--\", label=f\"Median: {df[col].median():.2f}\")\n",
    "\n",
    "        axes[i].set_title(f\"Distribution of {col}\")\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel(\"Density\")\n",
    "        axes[i].legend()\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(numerical_cols), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution statistics\n",
    "print(\"\\n📈 Distribution Characteristics:\")\n",
    "for col in numerical_cols:\n",
    "    skew = df[col].skew()\n",
    "    kurt = df[col].kurtosis()\n",
    "\n",
    "    skew_interpretation = \"Right-skewed\" if skew > 0.5 else \"Left-skewed\" if skew < -0.5 else \"Approximately symmetric\"\n",
    "    kurt_interpretation = \"Heavy-tailed\" if kurt > 3 else \"Light-tailed\" if kurt < 3 else \"Normal-tailed\"\n",
    "\n",
    "    print(f\"{col}: {skew_interpretation} (skew: {skew:.2f}), {kurt_interpretation} (kurtosis: {kurt:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Outlier Detection and Analysis\n",
    "print(\"🚨 OUTLIER DETECTION AND ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "\n",
    "def detect_outliers_zscore(data, column, threshold=3):\n",
    "    \"\"\"Detect outliers using Z-score method\"\"\"\n",
    "    z_scores = np.abs(stats.zscore(data[column]))\n",
    "    outliers = data[z_scores > threshold]\n",
    "    return outliers\n",
    "\n",
    "\n",
    "# Outlier analysis for each numerical column\n",
    "outlier_summary = []\n",
    "\n",
    "fig, axes = plt.subplots((len(numerical_cols) + 1) // 2, 2, figsize=(15, 6 * ((len(numerical_cols) + 1) // 2)))\n",
    "axes = axes.flatten() if len(numerical_cols) > 2 else [axes] if len(numerical_cols) == 2 else axes\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    if i < len(axes):\n",
    "        # IQR method\n",
    "        outliers_iqr, lower_bound, upper_bound = detect_outliers_iqr(df, col)\n",
    "\n",
    "        # Z-score method\n",
    "        outliers_zscore = detect_outliers_zscore(df, col)\n",
    "\n",
    "        # Store results\n",
    "        outlier_summary.append(\n",
    "            {\n",
    "                \"Column\": col,\n",
    "                \"IQR_Outliers\": len(outliers_iqr),\n",
    "                \"IQR_Percentage\": (len(outliers_iqr) / len(df)) * 100,\n",
    "                \"ZScore_Outliers\": len(outliers_zscore),\n",
    "                \"ZScore_Percentage\": (len(outliers_zscore) / len(df)) * 100,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Box plot\n",
    "        axes[i].boxplot(df[col], vert=True)\n",
    "        axes[i].set_title(f\"Box Plot: {col}\\nIQR Outliers: {len(outliers_iqr)} ({(len(outliers_iqr) / len(df) * 100):.1f}%)\")\n",
    "        axes[i].set_ylabel(col)\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(numerical_cols), len(axes)):\n",
    "    if i < len(axes):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Outlier summary table\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(\"\\n📊 Outlier Summary:\")\n",
    "print(outlier_df)\n",
    "\n",
    "# Extreme outliers (beyond 3 standard deviations)\n",
    "print(\"\\n🔍 Extreme Outliers Analysis:\")\n",
    "for col in numerical_cols:\n",
    "    extreme_outliers = df[np.abs(stats.zscore(df[col])) > 3]\n",
    "    if len(extreme_outliers) > 0:\n",
    "        print(f\"{col}: {len(extreme_outliers)} extreme outliers ({len(extreme_outliers) / len(df) * 100:.2f}%)\")\n",
    "        print(f\"  Values: {extreme_outliers[col].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Correlation Analysis\n",
    "print(\"🔗 CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(\n",
    "    correlation_matrix, mask=mask, annot=True, cmap=\"coolwarm\", center=0, square=True, fmt=\".2f\", cbar_kws={\"shrink\": 0.8}\n",
    ")\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Strong correlations (absolute value > 0.7)\n",
    "print(\"\\n🔥 Strong Correlations (|r| > 0.7):\")\n",
    "strong_corr = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            strong_corr.append(\n",
    "                {\n",
    "                    \"Variable 1\": correlation_matrix.columns[i],\n",
    "                    \"Variable 2\": correlation_matrix.columns[j],\n",
    "                    \"Correlation\": corr_val,\n",
    "                }\n",
    "            )\n",
    "\n",
    "if strong_corr:\n",
    "    strong_corr_df = pd.DataFrame(strong_corr)\n",
    "    print(strong_corr_df.sort_values(\"Correlation\", key=abs, ascending=False))\n",
    "else:\n",
    "    print(\"No strong correlations found.\")\n",
    "\n",
    "# Correlation with target variable\n",
    "if target_col in numerical_cols:\n",
    "    target_corr = correlation_matrix[target_col].drop(target_col).sort_values(key=abs, ascending=False)\n",
    "\n",
    "    print(f\"\\n🎯 Correlation with {target_col}:\")\n",
    "    print(target_corr)\n",
    "\n",
    "    # Visualize correlations with target\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    target_corr.plot(kind=\"barh\", color=[\"red\" if x < 0 else \"green\" for x in target_corr.values])\n",
    "    plt.title(f\"Correlation with {target_col}\")\n",
    "    plt.xlabel(\"Correlation Coefficient\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Feature Relationships and Scatter Plots\n",
    "print(\"🔍 FEATURE RELATIONSHIPS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Pairwise scatter plots for highly correlated features with target\n",
    "if target_col in numerical_cols:\n",
    "    # Get top correlated features with target\n",
    "    target_corr = df[numerical_cols].corrwith(df[target_col]).abs().sort_values(ascending=False)\n",
    "    top_features = target_corr.head(6).index.tolist()  # Top 6 including target\n",
    "\n",
    "    if target_col not in top_features:\n",
    "        top_features.append(target_col)\n",
    "\n",
    "    # Create pairplot\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    sns.pairplot(df[top_features], diag_kind=\"kde\", plot_kws={\"alpha\": 0.6})\n",
    "    plt.suptitle(\"Pairwise Relationships - Top Correlated Features\", y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    # Individual scatter plots with target\n",
    "    other_features = [col for col in top_features if col != target_col][:4]  # Top 4 features\n",
    "\n",
    "    if other_features:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i, feature in enumerate(other_features):\n",
    "            if i < 4:\n",
    "                # Scatter plot\n",
    "                axes[i].scatter(df[feature], df[target_col], alpha=0.6, color=\"blue\")\n",
    "\n",
    "                # Add regression line\n",
    "                z = np.polyfit(df[feature], df[target_col], 1)\n",
    "                p = np.poly1d(z)\n",
    "                axes[i].plot(df[feature], p(df[feature]), \"r--\", alpha=0.8)\n",
    "\n",
    "                # Calculate correlation\n",
    "                corr_coef = df[feature].corr(df[target_col])\n",
    "\n",
    "                axes[i].set_xlabel(feature)\n",
    "                axes[i].set_ylabel(target_col)\n",
    "                axes[i].set_title(f\"{feature} vs {target_col}\\nCorrelation: {corr_coef:.3f}\")\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Categorical vs Numerical Analysis\n",
    "print(\"🔀 CATEGORICAL vs NUMERICAL ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if len(categorical_cols) > 0 and len(numerical_cols) > 0:\n",
    "    for cat_col in categorical_cols:\n",
    "        print(f\"\\n📊 Analysis of {cat_col} vs Numerical Variables:\")\n",
    "\n",
    "        # Statistical tests\n",
    "        for num_col in numerical_cols[:4]:  # Limit to first 4 numerical columns\n",
    "            groups = [df[df[cat_col] == category][num_col].values for category in df[cat_col].unique()]\n",
    "\n",
    "            # ANOVA test\n",
    "            try:\n",
    "                f_stat, p_value = stats.f_oneway(*groups)\n",
    "                print(f\"{cat_col} vs {num_col}: F-statistic = {f_stat:.3f}, p-value = {p_value:.3f}\")\n",
    "            except:\n",
    "                print(f\"{cat_col} vs {num_col}: Could not perform ANOVA\")\n",
    "\n",
    "        # Visualizations\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        num_cols_subset = numerical_cols[:4]  # First 4 numerical columns\n",
    "\n",
    "        for i, num_col in enumerate(num_cols_subset):\n",
    "            if i < 4:\n",
    "                # Box plot\n",
    "                df.boxplot(column=num_col, by=cat_col, ax=axes[i])\n",
    "                axes[i].set_title(f\"{num_col} by {cat_col}\")\n",
    "                axes[i].set_xlabel(cat_col)\n",
    "                axes[i].set_ylabel(num_col)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Summary statistics by category\n",
    "        print(f\"\\n📈 Summary Statistics of Numerical Variables by {cat_col}:\")\n",
    "        summary_by_cat = df.groupby(cat_col)[numerical_cols[:4]].agg([\"mean\", \"median\", \"std\"])\n",
    "        print(summary_by_cat.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Advanced Statistical Analysis\n",
    "print(\"📊 ADVANCED STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Normality tests\n",
    "print(\"🔍 Normality Tests (Shapiro-Wilk):\")\n",
    "print(\"H0: Data is normally distributed\")\n",
    "print(\"H1: Data is not normally distributed\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "normality_results = []\n",
    "for col in numerical_cols:\n",
    "    # Sample for large datasets (Shapiro-Wilk works best with n < 5000)\n",
    "    sample_size = min(1000, len(df))\n",
    "    sample_data = df[col].sample(sample_size, random_state=42)\n",
    "\n",
    "    stat, p_value = stats.shapiro(sample_data)\n",
    "    is_normal = p_value > 0.05\n",
    "\n",
    "    normality_results.append({\"Variable\": col, \"Statistic\": stat, \"P-value\": p_value, \"Is_Normal\": is_normal})\n",
    "\n",
    "    print(f\"{col}: Statistic = {stat:.4f}, P-value = {p_value:.4f}, Normal: {is_normal}\")\n",
    "\n",
    "normality_df = pd.DataFrame(normality_results)\n",
    "\n",
    "# Anderson-Darling test for normality\n",
    "print(f\"\\n🔍 Anderson-Darling Test Results:\")\n",
    "for col in numerical_cols[:3]:  # Limit for space\n",
    "    result = stats.anderson(df[col], dist=\"norm\")\n",
    "    print(f\"{col}: Statistic = {result.statistic:.4f}\")\n",
    "    for i, critical_value in enumerate(result.critical_values):\n",
    "        significance_level = result.significance_level[i]\n",
    "        if result.statistic < critical_value:\n",
    "            print(f\"  At {significance_level}% significance: Data appears normal\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"  Data does not appear normal at standard significance levels\")\n",
    "\n",
    "# Variance homogeneity test (if categorical variables exist)\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"\\n🔍 Levene's Test for Variance Homogeneity:\")\n",
    "    for cat_col in categorical_cols:\n",
    "        for num_col in numerical_cols[:3]:  # Limit for space\n",
    "            groups = [df[df[cat_col] == category][num_col].values for category in df[cat_col].unique()]\n",
    "            try:\n",
    "                stat, p_value = stats.levene(*groups)\n",
    "                print(f\"{num_col} across {cat_col}: Levene statistic = {stat:.4f}, p-value = {p_value:.4f}\")\n",
    "            except:\n",
    "                print(f\"{num_col} across {cat_col}: Could not perform Levene's test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Dimensionality Reduction Analysis\n",
    "print(\"🔄 DIMENSIONALITY REDUCTION ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Prepare data for PCA\n",
    "numerical_data = df[numerical_cols].copy()\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(numerical_data)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Explained variance\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "print(\"📊 PCA Results:\")\n",
    "print(f\"Number of components: {len(explained_variance_ratio)}\")\n",
    "print(f\"Total variance explained by first 2 components: {cumulative_variance[1]:.3f}\")\n",
    "print(f\"Total variance explained by first 3 components: {cumulative_variance[2]:.3f}\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Scree plot\n",
    "axes[0, 0].plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, \"bo-\")\n",
    "axes[0, 0].set_title(\"Scree Plot\")\n",
    "axes[0, 0].set_xlabel(\"Principal Component\")\n",
    "axes[0, 0].set_ylabel(\"Explained Variance Ratio\")\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative variance plot\n",
    "axes[0, 1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, \"ro-\")\n",
    "axes[0, 1].axhline(y=0.8, color=\"green\", linestyle=\"--\", label=\"80% Variance\")\n",
    "axes[0, 1].axhline(y=0.95, color=\"orange\", linestyle=\"--\", label=\"95% Variance\")\n",
    "axes[0, 1].set_title(\"Cumulative Explained Variance\")\n",
    "axes[0, 1].set_xlabel(\"Number of Components\")\n",
    "axes[0, 1].set_ylabel(\"Cumulative Explained Variance\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# PCA scatter plot (first 2 components)\n",
    "scatter = axes[1, 0].scatter(pca_result[:, 0], pca_result[:, 1], c=df[target_col], cmap=\"viridis\", alpha=0.6)\n",
    "axes[1, 0].set_title(\"PCA: First Two Components\")\n",
    "axes[1, 0].set_xlabel(f\"PC1 ({explained_variance_ratio[0]:.3f} variance)\")\n",
    "axes[1, 0].set_ylabel(f\"PC2 ({explained_variance_ratio[1]:.3f} variance)\")\n",
    "plt.colorbar(scatter, ax=axes[1, 0], label=target_col)\n",
    "\n",
    "# Component loadings heatmap\n",
    "components_df = pd.DataFrame(\n",
    "    pca.components_[:4].T,  # First 4 components\n",
    "    columns=[f\"PC{i + 1}\" for i in range(4)],\n",
    "    index=numerical_cols,\n",
    ")\n",
    "\n",
    "sns.heatmap(components_df, annot=True, cmap=\"coolwarm\", center=0, ax=axes[1, 1])\n",
    "axes[1, 1].set_title(\"PCA Component Loadings\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Component interpretation\n",
    "print(\"\\n🔍 Component Loadings (First 3 Components):\")\n",
    "for i in range(min(3, len(pca.components_))):\n",
    "    print(f\"\\nPC{i + 1} (Explains {explained_variance_ratio[i]:.3f} of variance):\")\n",
    "    loadings = pd.Series(pca.components_[i], index=numerical_cols)\n",
    "    top_loadings = loadings.abs().sort_values(ascending=False).head()\n",
    "    for feature, loading in top_loadings.items():\n",
    "        print(f\"  {feature}: {loadings[feature]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Data Quality and Recommendations\n",
    "print(\"✅ DATA QUALITY SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Data quality summary\n",
    "print(\"📋 DATA QUALITY SUMMARY:\")\n",
    "print(f\"✓ Dataset shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"✓ Missing values: {df.isnull().sum().sum()} total\")\n",
    "print(f\"✓ Duplicate rows: {df.duplicated().sum()}\")\n",
    "print(f\"✓ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Feature summary\n",
    "print(f\"\\n📊 FEATURE SUMMARY:\")\n",
    "print(f\"✓ Numerical features: {len(numerical_cols)}\")\n",
    "print(f\"✓ Categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "# Distribution summary\n",
    "normal_features = sum([result[\"Is_Normal\"] for result in normality_results])\n",
    "print(f\"✓ Normally distributed features: {normal_features}/{len(numerical_cols)}\")\n",
    "\n",
    "# Correlation insights\n",
    "if target_col in numerical_cols:\n",
    "    target_corr = df[numerical_cols].corrwith(df[target_col]).abs().sort_values(ascending=False)\n",
    "    high_corr_features = sum(target_corr > 0.5) - 1  # Exclude target itself\n",
    "    print(f\"✓ Features highly correlated with target (|r| > 0.5): {high_corr_features}\")\n",
    "\n",
    "print(f\"\\n🔍 KEY INSIGHTS:\")\n",
    "\n",
    "# Target variable insights\n",
    "if target_col in df.columns:\n",
    "    print(f\"• {target_col} distribution: Mean = {df[target_col].mean():.2f}, Std = {df[target_col].std():.2f}\")\n",
    "\n",
    "    skew = df[target_col].skew()\n",
    "    if abs(skew) > 0.5:\n",
    "        skew_direction = \"right\" if skew > 0 else \"left\"\n",
    "        print(f\"• {target_col} is {skew_direction}-skewed (skewness = {skew:.2f})\")\n",
    "\n",
    "# Outlier insights\n",
    "total_outliers = sum([result[\"IQR_Outliers\"] for result in outlier_summary])\n",
    "print(f\"• Total outliers detected (IQR method): {total_outliers}\")\n",
    "\n",
    "# Feature relationship insights\n",
    "if target_col in numerical_cols:\n",
    "    target_corr = df[numerical_cols].corrwith(df[target_col]).abs().sort_values(ascending=False)\n",
    "    top_feature = target_corr.index[1] if len(target_corr) > 1 else None\n",
    "    if top_feature:\n",
    "        print(f\"• Strongest predictor: {top_feature} (correlation = {target_corr[top_feature]:.3f})\")\n",
    "\n",
    "print(f\"\\n💡 RECOMMENDATIONS:\")\n",
    "\n",
    "# Missing data recommendations\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    print(\"• Handle missing values before modeling\")\n",
    "\n",
    "# Outlier recommendations\n",
    "if total_outliers > len(df) * 0.05:  # More than 5% outliers\n",
    "    print(\"• Consider outlier treatment (>5% of data points are outliers)\")\n",
    "\n",
    "# Normality recommendations\n",
    "non_normal_features = len(numerical_cols) - normal_features\n",
    "if non_normal_features > 0:\n",
    "    print(f\"• Consider data transformation for {non_normal_features} non-normal features\")\n",
    "\n",
    "# Correlation recommendations\n",
    "if target_col in numerical_cols:\n",
    "    target_corr = df[numerical_cols].corrwith(df[target_col]).abs().sort_values(ascending=False)\n",
    "    weak_features = sum(target_corr < 0.1) - (1 if target_col in target_corr.index else 0)\n",
    "    if weak_features > 0:\n",
    "        print(f\"• Consider feature selection: {weak_features} features have weak correlation with target\")\n",
    "\n",
    "# PCA recommendations\n",
    "variance_80_components = np.argmax(cumulative_variance >= 0.8) + 1\n",
    "if variance_80_components < len(numerical_cols):\n",
    "    print(f\"• Dimensionality reduction possible: {variance_80_components} components explain 80% variance\")\n",
    "\n",
    "print(f\"\\n🎯 MODELING SUGGESTIONS:\")\n",
    "print(\"• Consider ensemble methods due to feature complexity\")\n",
    "print(\"• Evaluate both linear and non-linear models\")\n",
    "print(\"• Use cross-validation for robust model evaluation\")\n",
    "print(\"• Monitor for overfitting given the feature relationships\")\n",
    "\n",
    "print(f\"\\n📝 NEXT STEPS:\")\n",
    "print(\"1. Handle data quality issues (missing values, outliers)\")\n",
    "print(\"2. Feature engineering based on correlation insights\")\n",
    "print(\"3. Split data into train/validation/test sets\")\n",
    "print(\"4. Baseline model development\")\n",
    "print(\"5. Feature selection and hyperparameter tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Export Summary Report\n",
    "print(\"📄 GENERATING SUMMARY REPORT\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create a comprehensive summary dictionary\n",
    "summary_report = {\n",
    "    \"dataset_info\": {\n",
    "        \"shape\": df.shape,\n",
    "        \"memory_usage_mb\": df.memory_usage(deep=True).sum() / 1024**2,\n",
    "        \"missing_values\": df.isnull().sum().sum(),\n",
    "        \"duplicates\": df.duplicated().sum(),\n",
    "    },\n",
    "    \"feature_info\": {\n",
    "        \"numerical_features\": len(numerical_cols),\n",
    "        \"categorical_features\": len(categorical_cols),\n",
    "        \"total_features\": len(df.columns),\n",
    "    },\n",
    "    \"target_analysis\": {\n",
    "        \"target_variable\": target_col,\n",
    "        \"mean\": df[target_col].mean() if target_col in df.columns else None,\n",
    "        \"std\": df[target_col].std() if target_col in df.columns else None,\n",
    "        \"skewness\": df[target_col].skew() if target_col in df.columns else None,\n",
    "        \"range\": [df[target_col].min(), df[target_col].max()] if target_col in df.columns else None,\n",
    "    },\n",
    "    \"correlation_insights\": {\n",
    "        \"strong_correlations\": len(strong_corr) if \"strong_corr\" in locals() else 0,\n",
    "        \"top_predictor\": target_corr.index[1] if target_col in numerical_cols and len(target_corr) > 1 else None,\n",
    "        \"top_correlation\": float(target_corr.iloc[1]) if target_col in numerical_cols and len(target_corr) > 1 else None,\n",
    "    },\n",
    "    \"data_quality\": {\n",
    "        \"outlier_percentage\": (total_outliers / len(df)) * 100,\n",
    "        \"normal_features\": normal_features,\n",
    "        \"non_normal_features\": len(numerical_cols) - normal_features,\n",
    "    },\n",
    "    \"pca_insights\": {\n",
    "        \"components_for_80_variance\": int(variance_80_components),\n",
    "        \"first_two_components_variance\": float(cumulative_variance[1]),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save summary to file\n",
    "import json\n",
    "\n",
    "with open(\"../data/eda_summary.json\", \"w\") as f:\n",
    "    json.dump(summary_report, f, indent=2, default=str)\n",
    "\n",
    "print(\"✅ EDA Summary saved to '../data/eda_summary.json'\")\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\n🎉 EDA COMPLETE!\")\n",
    "print(\"=\" * 20)\n",
    "print(\"Key files generated:\")\n",
    "print(\"• EDA summary: ../data/eda_summary.json\")\n",
    "print(\"\\nThis comprehensive EDA provides insights for:\")\n",
    "print(\"• Data preprocessing decisions\")\n",
    "print(\"• Feature engineering strategies\")\n",
    "print(\"• Model selection guidance\")\n",
    "print(\"• Performance evaluation metrics\")\n",
    "\n",
    "print(f\"\\n📱 Dataset Overview:\")\n",
    "print(f\"• {df.shape[0]:,} samples with {df.shape[1]} features\")\n",
    "print(f\"• Target variable: {target_col}\")\n",
    "print(f\"• Ready for ML pipeline development! 🚀\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
